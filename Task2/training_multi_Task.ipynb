{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "N_WORKERS = os.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from comet_ml import Experiment\n",
    "\n",
    "\n",
    "# # Create an instance of the Experiment class\n",
    "# experiment = Experiment(\n",
    "#     project_name=\"ResNet18 Piece Counter Combination\",  # Replace with your project name\n",
    "#     workspace=\"cristy17001\"  # Replace with your workspace name\n",
    "# )\n",
    "\n",
    "# experiment.set_name(\"ResNet18 Piece Counter Combination 1\")\n",
    "# experiment.log_parameters({\n",
    "#     \"model\": \"resnet18\",\n",
    "#     \"optimizer\": \"AdamW\",\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"weight_decay\": 1e-4,\n",
    "#     \"loss_function\": \"BCE + MSE\",\n",
    "#     \"scheduler\": \"ReduceLROnPlateau\",\n",
    "#     \"pretrained\": True,\n",
    "#     \"patience\": 2,\n",
    "#     \"batch_size\": 64,\n",
    "#     \"epochs\": N_EPOCHS,\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PreloadedDataset(Dataset):\n",
    "    def __init__(self, tensor_file):\n",
    "        self.data = torch.load(tensor_file)  # list of (img_tensor, presence_tensor, count_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_tensor, presence_tensor, count_tensor = self.data[idx]\n",
    "        return img_tensor, presence_tensor, count_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristiano\\AppData\\Local\\Temp\\ipykernel_7324\\592161780.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(tensor_file)  # list of (img_tensor, presence_tensor, count_tensor)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PreloadedDataset(\"./split_count_presence/train_data.pt\")\n",
    "test_dataset = PreloadedDataset(\"./split_count_presence/test_data.pt\")\n",
    "val_dataset = PreloadedDataset(\"./split_count_presence/val_data.pt\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18MultiTask(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNet18MultiTask, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-50\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "\n",
    "        # Remove the classification head (fc layer)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Output: [B, 512, 1, 1]\n",
    "\n",
    "        # Flatten layer (ResNet output is [B, 512, 1, 1])\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Classification head for presence map (64 outputs for 8x8 grid)\n",
    "        self.presence_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "        )\n",
    "\n",
    "        # Regression head for piece count\n",
    "        self.count_head = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output is a scalar\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)  # [B, 512, 1, 1]\n",
    "        features = self.flatten(features)     # [B, 512]\n",
    "        \n",
    "        presence_out = self.presence_head(features)  # [B, 64]\n",
    "        count_out = self.count_head(features)        # [B, 1]\n",
    "\n",
    "        return presence_out, count_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristiano\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Cristiano\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18MultiTask().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "count_loss = nn.L1Loss()\n",
    "presence_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "PRESENCE_WEIGHT = 0.7\n",
    "COUNT_WEIGHT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cristiano\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.2425\n",
      "Validation Loss: 4.0902 | Presence Accuracy: 0.7009 | Count MAE: 13.7282\n",
      "Epoch 2, Train Loss: 4.8904\n",
      "Validation Loss: 2.9434 | Presence Accuracy: 0.7182 | Count MAE: 9.7101\n",
      "Epoch 3, Train Loss: 3.4143\n",
      "Validation Loss: 1.9037 | Presence Accuracy: 0.7374 | Count MAE: 5.7177\n",
      "Epoch 4, Train Loss: 1.7841\n",
      "Validation Loss: 1.0231 | Presence Accuracy: 0.7455 | Count MAE: 2.1537\n",
      "Epoch 5, Train Loss: 0.9478\n",
      "Validation Loss: 0.8706 | Presence Accuracy: 0.7590 | Count MAE: 1.6201\n",
      "Epoch 6, Train Loss: 0.7745\n",
      "Validation Loss: 0.9190 | Presence Accuracy: 0.7636 | Count MAE: 1.8725\n",
      "Epoch 7, Train Loss: 0.7267\n",
      "Validation Loss: 0.7680 | Presence Accuracy: 0.7647 | Count MAE: 1.4631\n",
      "Epoch 8, Train Loss: 0.6899\n",
      "Validation Loss: 0.7267 | Presence Accuracy: 0.7695 | Count MAE: 1.3412\n",
      "Epoch 9, Train Loss: 0.6404\n",
      "Validation Loss: 0.7591 | Presence Accuracy: 0.7706 | Count MAE: 1.4090\n",
      "Epoch 10, Train Loss: 0.6438\n",
      "Validation Loss: 0.7003 | Presence Accuracy: 0.7711 | Count MAE: 1.2374\n",
      "Epoch 11, Train Loss: 0.6726\n",
      "Validation Loss: 0.6752 | Presence Accuracy: 0.7724 | Count MAE: 1.1855\n",
      "Epoch 12, Train Loss: 0.6325\n",
      "Validation Loss: 0.6931 | Presence Accuracy: 0.7731 | Count MAE: 1.2277\n",
      "Epoch 13, Train Loss: 0.6117\n",
      "Validation Loss: 0.6909 | Presence Accuracy: 0.7737 | Count MAE: 1.2363\n",
      "Epoch 14, Train Loss: 0.5968\n",
      "Validation Loss: 0.6974 | Presence Accuracy: 0.7736 | Count MAE: 1.2159\n",
      "Epoch 15, Train Loss: 0.5598\n",
      "Validation Loss: 0.6553 | Presence Accuracy: 0.7731 | Count MAE: 1.0946\n",
      "Epoch 16, Train Loss: 0.5629\n",
      "Validation Loss: 0.6677 | Presence Accuracy: 0.7732 | Count MAE: 1.1432\n",
      "Epoch 17, Train Loss: 0.5339\n",
      "Validation Loss: 0.6523 | Presence Accuracy: 0.7733 | Count MAE: 1.0951\n",
      "Epoch 18, Train Loss: 0.5618\n",
      "Validation Loss: 0.6534 | Presence Accuracy: 0.7731 | Count MAE: 1.0878\n",
      "Epoch 19, Train Loss: 0.5274\n",
      "Validation Loss: 0.6541 | Presence Accuracy: 0.7737 | Count MAE: 1.1051\n",
      "Epoch 20, Train Loss: 0.5426\n",
      "Validation Loss: 0.6457 | Presence Accuracy: 0.7735 | Count MAE: 1.0610\n",
      "Epoch 21, Train Loss: 0.5410\n",
      "Validation Loss: 0.6617 | Presence Accuracy: 0.7733 | Count MAE: 1.1156\n",
      "Epoch 22, Train Loss: 0.5417\n",
      "Validation Loss: 0.6652 | Presence Accuracy: 0.7732 | Count MAE: 1.1268\n",
      "Epoch 23, Train Loss: 0.5103\n",
      "Validation Loss: 0.6601 | Presence Accuracy: 0.7737 | Count MAE: 1.1099\n",
      "Epoch 24, Train Loss: 0.5279\n",
      "Validation Loss: 0.6632 | Presence Accuracy: 0.7738 | Count MAE: 1.1039\n",
      "Epoch 25, Train Loss: 0.5221\n",
      "Validation Loss: 0.6644 | Presence Accuracy: 0.7729 | Count MAE: 1.1490\n",
      "Epoch 26, Train Loss: 0.5040\n",
      "Validation Loss: 0.6644 | Presence Accuracy: 0.7737 | Count MAE: 1.1083\n",
      "Epoch 27, Train Loss: 0.5107\n",
      "Validation Loss: 0.6660 | Presence Accuracy: 0.7737 | Count MAE: 1.1205\n",
      "Epoch 28, Train Loss: 0.5773\n",
      "Validation Loss: 0.6621 | Presence Accuracy: 0.7726 | Count MAE: 1.1408\n",
      "Epoch 29, Train Loss: 0.5441\n",
      "Validation Loss: 0.6583 | Presence Accuracy: 0.7738 | Count MAE: 1.1011\n",
      "Epoch 30, Train Loss: 0.5224\n",
      "Validation Loss: 0.6694 | Presence Accuracy: 0.7735 | Count MAE: 1.1183\n",
      "Epoch 31, Train Loss: 0.5734\n",
      "Validation Loss: 0.6566 | Presence Accuracy: 0.7736 | Count MAE: 1.0976\n",
      "Epoch 32, Train Loss: 0.5386\n",
      "Validation Loss: 0.6572 | Presence Accuracy: 0.7734 | Count MAE: 1.1077\n",
      "Epoch 33, Train Loss: 0.5186\n",
      "Validation Loss: 0.6626 | Presence Accuracy: 0.7736 | Count MAE: 1.1085\n",
      "Epoch 34, Train Loss: 0.5426\n",
      "Validation Loss: 0.6594 | Presence Accuracy: 0.7740 | Count MAE: 1.0978\n",
      "Epoch 35, Train Loss: 0.5498\n",
      "Validation Loss: 0.6559 | Presence Accuracy: 0.7739 | Count MAE: 1.0959\n",
      "Epoch 36, Train Loss: 0.5127\n",
      "Validation Loss: 0.6591 | Presence Accuracy: 0.7734 | Count MAE: 1.1070\n",
      "Epoch 37, Train Loss: 0.5751\n",
      "Validation Loss: 0.6573 | Presence Accuracy: 0.7736 | Count MAE: 1.1049\n",
      "Epoch 38, Train Loss: 0.5136\n",
      "Validation Loss: 0.6654 | Presence Accuracy: 0.7733 | Count MAE: 1.1231\n",
      "Epoch 39, Train Loss: 0.5182\n",
      "Validation Loss: 0.6598 | Presence Accuracy: 0.7740 | Count MAE: 1.0976\n",
      "Epoch 40, Train Loss: 0.5851\n",
      "Validation Loss: 0.6617 | Presence Accuracy: 0.7731 | Count MAE: 1.1275\n",
      "Epoch 41, Train Loss: 0.5731\n",
      "Validation Loss: 0.6633 | Presence Accuracy: 0.7738 | Count MAE: 1.1141\n",
      "Epoch 42, Train Loss: 0.5411\n",
      "Validation Loss: 0.6521 | Presence Accuracy: 0.7733 | Count MAE: 1.1078\n",
      "Epoch 43, Train Loss: 0.5105\n",
      "Validation Loss: 0.6587 | Presence Accuracy: 0.7734 | Count MAE: 1.1181\n",
      "Epoch 44, Train Loss: 0.5309\n",
      "Validation Loss: 0.6626 | Presence Accuracy: 0.7739 | Count MAE: 1.1172\n",
      "Epoch 45, Train Loss: 0.5316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m all_count_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_maps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpresence_maps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, presence_maps, counts in train_loader:\n",
    "        images = images.to(device)\n",
    "        presence_maps = presence_maps.to(device)\n",
    "        counts = counts.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs_presence, outputs_count = model(images)\n",
    "        loss_presence = presence_loss(outputs_presence, presence_maps)\n",
    "        loss_count = count_loss(outputs_count.squeeze(1), counts)\n",
    "        loss = PRESENCE_WEIGHT * loss_presence + COUNT_WEIGHT * loss_count\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_presence_preds = []\n",
    "    all_presence_labels = []\n",
    "    all_count_preds = []\n",
    "    all_count_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, presence_maps, counts in validation_loader:\n",
    "            images = images.to(device)\n",
    "            presence_maps = presence_maps.to(device)\n",
    "            counts = counts.to(device)\n",
    "\n",
    "            presence_pred, count_pred = model(images)\n",
    "\n",
    "            loss_presence = presence_loss(presence_pred, presence_maps)\n",
    "            loss_count = count_loss(count_pred.squeeze(1), counts)  # Fix shape here\n",
    "            val_loss += (PRESENCE_WEIGHT * loss_presence + COUNT_WEIGHT * loss_count).item()  # Use same weights as training\n",
    "\n",
    "            # Collect for metrics\n",
    "            all_presence_preds.append(presence_pred.detach().cpu())\n",
    "            all_presence_labels.append(presence_maps.detach().cpu())\n",
    "            all_count_preds.append(count_pred.detach().cpu())\n",
    "            all_count_labels.append(counts.detach().cpu())\n",
    "\n",
    "    val_loss /= len(validation_loader)\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_presence_preds = torch.cat(all_presence_preds).numpy()\n",
    "    all_presence_labels = torch.cat(all_presence_labels).numpy()\n",
    "    all_count_preds = torch.cat(all_count_preds).numpy()\n",
    "    all_count_labels = torch.cat(all_count_labels).numpy()\n",
    "\n",
    "    # Convert presence predictions to binary by thresholding at 0.5\n",
    "    all_presence_preds_binary = (all_presence_preds > 0.5).astype(int)\n",
    "    all_presence_labels_int = all_presence_labels.astype(int)\n",
    "\n",
    "    # Accuracy for presence map\n",
    "    presence_accuracy = accuracy_score(all_presence_labels_int.flatten(), all_presence_preds_binary.flatten())\n",
    "\n",
    "    # MAE for count regression\n",
    "    count_mae = mean_absolute_error(all_count_labels, all_count_preds)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Presence Accuracy: {presence_accuracy:.4f} | Count MAE: {count_mae:.4f}\")\n",
    "    scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
